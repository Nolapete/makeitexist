[
  {
    "model": "blog.category",
    "pk": 1,
    "fields": {
      "name": "Development",
      "slug": "development"
    }
  },
  {
    "model": "blog.post",
    "pk": 1,
    "fields": {
      "title": "Bringing Dev Activity to Life: Building a Dynamic GitHub Feed in Django with Celery",
      "slug": "building-github-activity-feed-django-celery",
      "content": "## Introduction\n\nEvery developer wants to show off their recent work, but manually updating a \"recent projects\" list is tedious. We wanted a live feed of our GitHub commits directly on `makeitexist.net`.\n\nThe challenge? Fetching data from an external API (like GitHub's) live on every page load can slow down your site significantly. We needed a reliable, asynchronous solution.\n\nWe combined **Django** for the web app, **Celery** and **Redis** for background processing, and the **GitHub REST API** for the data. This ensures the user experience remains fast while the data syncs reliably in the background.\n\n### Phase 1: Setting up the Data Pipeline\n\nWe needed a place to store the GitHub data locally so our website could read from its own database instead of constantly hitting GitHub's servers.\n\n#### Django Models\n\nWe created simple Django models (`Repository` and `Commit`) to structure the incoming JSON data in our database:\n\n```python\n# github_feed/models.py\nfrom django.db import models\n\nclass Repository(models.Model):\n    # ... fields for repo name, owner, URL ...\n    pass\n\nclass Commit(models.Model):\n    \"\"\"Stores individual commit information.\"\"\"\n    sha = models.CharField(max_length=40, unique=True, primary_key=True)\n    repository = models.ForeignKey(Repository, on_delete=models.CASCADE, related_name='commits')\n    message = models.TextField()\n    author_name = models.CharField(max_length=100)\n    date = models.DateTimeField()\n    html_url = models.URLField()\n\n    class Meta:\n        # Default ordering: newest first, then by repo name\n        ordering = ['-date', 'repository__name']\n```\n\n#### Secure Authentication\n\nWe stored our GitHub Personal Access Token (PAT) securely in a `.env` file using `django-environ` to keep credentials out of our code:\n\n```\n# .env file\nGITHUB_PAT=ghp_YourActualTokenHere12345\nGITHUB_USERNAME=your_github_username\n```\n\n### Phase 2: The Magic of Asynchronous Tasks (Celery & Redis)\n\nThe core challenge was fetching *all* commits from *all* repositories without blocking the website. We configured a Celery worker process that runs independently of the main Django server.\n\nWe wrote Python tasks in `github_feed/tasks.py` to handle the heavy lifting:\n\n```python\n# github_feed/tasks.py snippet\n\n@shared_task\ndef sync_all_github_data():\n    \"\"\"Main task to orchestrate syncing all repositories.\"\"\"\n    # ... logic to fetch list of repositories from GitHub API ...\n    repositories_data = fetch_paginated_data(repos_url)\n    \n    for repo_data in repositories_data:\n        # Queue up individual tasks for each repository\n        fetch_commits_for_repo.delay(repo_instance.repo_id, repo_data['commits_url'])\n\n@shared_task\ndef fetch_commits_for_repo(repo_id, commits_url):\n    \"\"\"Fetches all commits for a specific repository instance.\"\"\"\n    # ... logic to fetch paginated commits and save to Commit model ...\n    pass\n```\n\n### Phase 3: Presentation and Organization\n\nOnce the data was in the database, we needed to display it clearly on our main landing page. We chose to group the data by **Date** and then **Repository**.\n\n#### The View Logic\n\nWe used Python's `defaultdict` within our `landing/views.py` to organize the flat list of commits into the desired hierarchy:\n\n```python\n# landing/views.py snippet for context processing\n    # Group commits by Date Object -> Repo Name -> List of Commits\n    grouped_commits = defaultdict(lambda: defaultdict(list))\n    # ... (processing loops and sorting logic) ...\n    context = { \"commits_by_date_and_repo\": sorted_commits_display_data }\n```\n\n#### The Template\n\nWe then used nested Django template loops (`{% for %}`) to render this organized data structure cleanly in `landing/landing.html`:\n\n```html\n<!-- landing/landing.html snippet for the feed section -->\n<div class=\"content-section github-feed-section\">\n    <div class=\"container\">\n        <!-- Loop 1: Group by Date -->\n        {% for date_obj, repos in commits_by_date_and_repo.items %} \n            <div class=\"date-group\">\n                <h3>{{ date_obj|date:\"l, M d, Y\" }}</h3>\n                \n                <!-- Loop 2: Group by Repository -->\n                {% for repo_name, commits in repos.items %}\n                    <!-- ... (inner loops and display logic) ... -->\n                    <div class=\"repo-group\">\n                        <h4>\n                            Repository: \n                            <a href=\"{{ commits.0.repository.html_url }}\" target=\"_blank\">{{ repo_name }}</a>\n                        </h4>\n                        <ul class=\"commit-list\">\n                            {% for commit in commits %}\n                                <li class=\"commit-item\">\n                                    <span class=\"commit-sha\">\n                                        <code><a href=\"{{ commit.html_url }}\" target=\"_blank\">{{ commit.sha|slice:\":7\" }}</a></code>\n                                    </span>\n                                    <span class=\"commit-separator\">â€”</span>\n                                    <span class=\"commit-message\">\n                                        {{ commit.message|truncatechars:120 }}\n                                    </span>\n                                </li>\n                            {% endfor %}\n                        </ul>\n                    </div>\n                {% endfor %}\n            </div>\n            <hr>\n        {% endfor %}\n    </div>\n</div>\n```\n\n### Conclusion\n\nBy decoupling our frontend display from the backend data fetching using Celery tasks, we achieved a responsive, real-time activity feed. The final configuration uses UTC time consistently across the server stack, ensuring clarity and robustness.",
      "pub_date": "2025-11-04T03:00:00Z",
      "author": 1,
      "category": 1,
      "is_published": true,
      "excerpt": null
    }
  }
]
